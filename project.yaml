metadata:
  version: 1.0.0
  timestamp: 2025-07-15 10:59:21.137360
  description: YAML snapshot of /Users/richardhightower/src/art_hug_09
  generator: yaml-project
  generator_version: 0.1.0
  author:
  tags:
  custom:
config:
  supported_extensions:
    .py: python
    .sh: bash
    .java: java
    .js: javascript
    .jsx: javascript
    .ts: typescript
    .tsx: typescript
    .html: html
    .css: css
    .md: markdown
    .yml: yaml
    .yaml: yaml
    .json: json
    .txt: text
    .go: go
    .rs: rust
    .rb: ruby
    .php: php
    .c: c
    .cpp: cpp
    .h: c
    .hpp: cpp
    .cs: csharp
    .toml: toml
    .xml: xml
    .sql: sql
    .kt: kotlin
    .swift: swift
    .dart: dart
    .r: r
    .scala: scala
    .pl: perl
    .lua: lua
    .ini: ini
    .cfg: ini
    .properties: properties
  forbidden_dirs:
    - __pycache__
    - node_modules
    - dist
    - cdk.out
    - env
    - venv
    - .venv
    - .idea
    - build
    - .git
    - .svn
    - .hg
    - .DS_Store
    - .vs
    - .vscode
    - target
    - bin
    - obj
    - out
    - Debug
    - Release
    - tmp
    - .tox
    - .pytest_cache
    - __MACOSX
    - .mypy_cache
    - tests
  include_pattern:
  exclude_pattern:
  outfile: /Users/richardhightower/src/art_hug_09/project.yaml
  log_level: INFO
  max_file_size: 204800
  config_dir: .yamlproject
  chunking_enabled: false
  chunk_size: 1048576
  temp_dir:
  backup_enabled: false
  backup_dir:
  metadata_fields:
    - extension
    - size_bytes
    - language
  yaml_format:
    indent: 2
    width: 120
tests:
content:
  files:
    pyproject.toml:
      content: |
        [tool.poetry]
        name = "model-optimization"
        version = "0.1.0"
        description = "Model Optimization and Deployment - Working Examples"
        authors = ["Your Name <you@example.com>"]
        readme = "README.md"
        packages = [{include = "src"}]

        [tool.poetry.dependencies]
        python = "^3.12"
        transformers = "^{latest}"
        onnx = "^{latest}"
        onnxruntime = "^{latest}"
        optimum = "^{latest}"
        python-dotenv = "^1.0.0"
        pandas = "^2.1.0"
        numpy = "^1.26.0"

        [tool.poetry.group.dev.dependencies]
        pytest = "^8.0.0"
        black = "^24.0.0"
        ruff = "^0.6.0"
        jupyter = "^1.0.0"
        ipykernel = "^6.29.0"

        [build-system]
        requires = ["poetry-core"]
        build-backend = "poetry.core.masonry.api"

        [tool.black]
        line-length = 88
        target-version = ['py312']

        [tool.ruff]
        line-length = 88
        target-version = "py312"

        [tool.ruff.lint]
        select = ["E", "F", "I", "N", "UP", "B", "C4", "SIM"]
      metadata:
        extension: .toml
        size_bytes: 824
        language: toml
    README.md:
      content: |
        # Model Optimization and Deployment

        This project contains working examples for Chapter 09 of the Hugging Face Transformers book.

        ## Overview

        Learn how to implement and understand:

        ## Prerequisites

        - Python 3.12 (managed via pyenv)
        - Poetry for dependency management
        - Go Task for build automation
        - API keys for any required services (see .env.example)

        ## Setup

        1. Clone this repository
        2. Run the setup task:
           ```bash
           task setup
           ```
        3. Copy `.env.example` to `.env` and configure as needed

        ## Project Structure

        ```
        .
        ├── src/
        │   ├── __init__.py
        │   ├── config.py              # Configuration and utilities
        │   ├── main.py                # Entry point with all examples
        │   ├── quantization.py        # Quantization implementation
        │   ├── pruning.py        # Pruning implementation
        │   ├── onnx_export.py        # Onnx Export implementation
        │   ├── performance_benchmarking.py        # Performance Benchmarking implementation
        │   └── utils.py               # Utility functions
        ├── tests/
        │   └── test_examples.py       # Unit tests
        ├── .env.example               # Environment template
        ├── Taskfile.yml               # Task automation
        └── pyproject.toml             # Poetry configuration
        ```

        ## Running Examples

        Run all examples:
        ```bash
        task run
        ```

        Or run individual modules:
        ```bash
        task run-quantization    # Run quantization
        task run-pruning    # Run pruning
        task run-onnx-export    # Run onnx export
        ```

        ## Available Tasks

        - `task setup` - Set up Python environment and install dependencies
        - `task run` - Run all examples
        - `task test` - Run unit tests
        - `task format` - Format code with Black and Ruff
        - `task clean` - Clean up generated files

        ## Learn More

        - [Hugging Face Documentation](https://huggingface.co/docs)
        - [Transformers Library](https://github.com/huggingface/transformers)
        - [Book Resources](https://example.com/book-resources)
      metadata:
        extension: .md
        size_bytes: 1979
        language: markdown
    ch_09_project.yaml:
      content: |
        metadata:
          version: "1.0.0"
          timestamp: "2025-07-01T22:50:15.157293"
          description: "Chapter 09: Semantic Search and Information Retrieval with Transformers"
          generator: "yaml-project"
          generator_version: "0.1.0"
          author: "Hugging Face Transformers Book - Chapter 09"
          tags:
            - "transformers"
            - "nlp"
            - "chapter-09"
            - "example"

        content:
          files:
            pyproject.toml:
              content: |
                [tool.poetry]
                name = "model-optimization"
                version = "0.1.0"
                description = "Model Optimization and Deployment - Working Examples"
                authors = ["Your Name <you@example.com>"]
                readme = "README.md"
                packages = [{include = "src"}]

                [tool.poetry.dependencies]
                python = "^3.12"
                transformers = "^{latest}"
                onnx = "^{latest}"
                onnxruntime = "^{latest}"
                optimum = "^{latest}"
                python-dotenv = "^1.0.0"
                pandas = "^2.1.0"
                numpy = "^1.26.0"

                [tool.poetry.group.dev.dependencies]
                pytest = "^8.0.0"
                black = "^24.0.0"
                ruff = "^0.6.0"
                jupyter = "^1.0.0"
                ipykernel = "^6.29.0"

                [build-system]
                requires = ["poetry-core"]
                build-backend = "poetry.core.masonry.api"

                [tool.black]
                line-length = 88
                target-version = ['py312']

                [tool.ruff]
                line-length = 88
                target-version = "py312"

                [tool.ruff.lint]
                select = ["E", "F", "I", "N", "UP", "B", "C4", "SIM"]
              metadata:
                extension: .toml
                size_bytes: 1000
                language: toml
            README.md:
              content: |
                # Model Optimization and Deployment

                This project contains working examples for Chapter 09 of the Hugging Face Transformers book.

                ## Overview

                Learn how to implement and understand:

                ## Prerequisites

                - Python 3.12 (managed via pyenv)
                - Poetry for dependency management
                - Go Task for build automation
                - API keys for any required services (see .env.example)

                ## Setup

                1. Clone this repository
                2. Run the setup task:
                   ```bash
                   task setup
                   ```
                3. Copy `.env.example` to `.env` and configure as needed

                ## Project Structure

                ```
                .
                ├── src/
                │   ├── __init__.py
                │   ├── config.py              # Configuration and utilities
                │   ├── main.py                # Entry point with all examples
                │   ├── quantization.py        # Quantization implementation
                │   ├── pruning.py        # Pruning implementation
                │   ├── onnx_export.py        # Onnx Export implementation
                │   ├── performance_benchmarking.py        # Performance Benchmarking implementation
                │   └── utils.py               # Utility functions
                ├── tests/
                │   └── test_examples.py       # Unit tests
                ├── .env.example               # Environment template
                ├── Taskfile.yml               # Task automation
                └── pyproject.toml             # Poetry configuration
                ```

                ## Running Examples

                Run all examples:
                ```bash
                task run
                ```

                Or run individual modules:
                ```bash
                task run-quantization    # Run quantization
                task run-pruning    # Run pruning
                task run-onnx-export    # Run onnx export
                ```

                ## Available Tasks

                - `task setup` - Set up Python environment and install dependencies
                - `task run` - Run all examples
                - `task test` - Run unit tests
                - `task format` - Format code with Black and Ruff
                - `task clean` - Clean up generated files

                ## Learn More

                - [Hugging Face Documentation](https://huggingface.co/docs)
                - [Transformers Library](https://github.com/huggingface/transformers)
                - [Book Resources](https://example.com/book-resources)
              metadata:
                extension: .md
                size_bytes: 2000
                language: markdown
            Taskfile.yml:
              content: |
                version: '3'

                vars:
                  PYTHON_VERSION: 3.12.9

                tasks:
                  default:
                    desc: "Show available tasks"
                    cmds:
                      - task --list

                  setup:
                    desc: "Set up the Python environment and install dependencies"
                    cmds:
                      - pyenv install -s {{.PYTHON_VERSION}}
                      - pyenv local {{.PYTHON_VERSION}}
                      - poetry install
                      - poetry config virtualenvs.in-project true
                      - 'echo "Setup complete! Activate with: source .venv/bin/activate"'

                  run:
                    desc: "Run all examples"
                    cmds:
                      - poetry run python src/main.py

                  run-quantization:
                    desc: "Run quantization examples"
                    cmds:
                      - poetry run python src/quantization.py

                  run-pruning:
                    desc: "Run pruning examples"
                    cmds:
                      - poetry run python src/pruning.py

                  run-onnx-export:
                    desc: "Run onnx export examples"
                    cmds:
                      - poetry run python src/onnx_export.py

                  test:
                    desc: "Run all tests"
                    cmds:
                      - poetry run pytest tests/ -v

                  format:
                    desc: "Format code with Black and Ruff"
                    cmds:
                      - poetry run black src/ tests/
                      - poetry run ruff check --fix src/ tests/

                  clean:
                    desc: "Clean up generated files"
                    cmds:
                      - find . -type d -name "__pycache__" -exec rm -rf {} +
                      - find . -type f -name "*.pyc" -delete
                      - rm -rf .pytest_cache
                      - rm -rf .ruff_cache
                      - rm -rf .mypy_cache
              metadata:
                extension: .yml
                size_bytes: 1200
                language: yaml
            src/__init__.py:
              content: |
                """
                Chapter 09 Examples: Model Optimization and Deployment
                """

                __version__ = "0.1.0"
              metadata:
                extension: .py
                size_bytes: 100
                language: python

            src/config.py:
              content: |
                """Configuration module for examples."""
                
                import os
                from pathlib import Path
                from dotenv import load_dotenv
                
                # Load environment variables
                load_dotenv()
                
                # Project paths
                PROJECT_ROOT = Path(__file__).parent.parent
                DATA_DIR = PROJECT_ROOT / "data"
                MODELS_DIR = PROJECT_ROOT / "models"
                
                # Create directories if they don't exist
                DATA_DIR.mkdir(exist_ok=True)
                MODELS_DIR.mkdir(exist_ok=True)
                
                # Model configurations
                DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "bert-base-uncased")
                BATCH_SIZE = int(os.getenv("BATCH_SIZE", "8"))
                MAX_LENGTH = int(os.getenv("MAX_LENGTH", "512"))
                
                # API keys (if needed)
                OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
                ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
                HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
                
                # Device configuration
                import torch
                
                def get_device():
                    """Get the best available device."""
                    if torch.backends.mps.is_available():
                        return "mps"
                    elif torch.cuda.is_available():
                        return "cuda"
                    else:
                        return "cpu"
                        
                DEVICE = get_device()
              metadata:
                extension: .py
                size_bytes: 1000
                language: python

            src/main.py:
              content: |
                """Main entry point for all examples."""
                
                import sys
                from pathlib import Path
                
                # Add src to path
                sys.path.append(str(Path(__file__).parent))
                
                from quantization import run_quantization_examples
                from pruning import run_pruning_examples
                from onnx_export import run_onnx_export_examples
                from performance_benchmarking import run_performance_benchmarking_examples
                
                def print_section(title: str):
                    """Print a formatted section header."""
                    print("\n" + "=" * 60)
                    print(f"  {title}")
                    print("=" * 60 + "\n")
                
                def main():
                    """Run all examples."""
                    print_section("CHAPTER 09: MODEL OPTIMIZATION AND DEPLOYMENT")
                    print("Welcome! This script demonstrates the concepts from this chapter.")
                    print("Each example builds on the previous concepts.\n")
                    
                    print_section("1. QUANTIZATION")
                    run_quantization_examples()
                    
                    print_section("2. PRUNING")
                    run_pruning_examples()
                    
                    print_section("3. ONNX EXPORT")
                    run_onnx_export_examples()
                    
                    print_section("CONCLUSION")
                    print("These examples demonstrate the key concepts from this chapter.")
                    print("Try modifying the code to experiment with different approaches!")
                
                if __name__ == "__main__":
                    main()
              metadata:
                extension: .py
                size_bytes: 1500
                language: python
            .env.example:
              content: |
                # Model Configuration
                DEFAULT_MODEL=bert-base-uncased
                BATCH_SIZE=8
                MAX_LENGTH=512
                
                # API Keys (optional, depending on chapter)
                OPENAI_API_KEY=your-openai-key-here
                ANTHROPIC_API_KEY=your-anthropic-key-here
                HUGGINGFACE_TOKEN=your-hf-token-here
                
                # Other Configuration
                LOG_LEVEL=INFO
                CACHE_DIR=~/.cache/transformers
              metadata:
                extension: .example
                size_bytes: 300
                language: text

            .gitignore:
              content: |
                # Python
                __pycache__/
                *.py[cod]
                *$py.class
                *.so
                .Python
                env/
                venv/
                .venv/
                ENV/
                .env
                
                # Poetry
                dist/
                *.egg-info/
                
                # Testing
                .pytest_cache/
                .coverage
                htmlcov/
                .tox/
                
                # IDE
                .idea/
                .vscode/
                *.swp
                *.swo
                .DS_Store
                
                # Project specific
                data/
                models/
                *.log
                .cache/
              metadata:
                extension: .gitignore
                size_bytes: 300
                language: text

            .python-version:
              content: |
                3.12.9
              metadata:
                extension: .python-version
                size_bytes: 7
                language: text
            src/quantization.py:
              content: |
                """Quantization implementation."""
                
                from transformers import pipeline, AutoTokenizer, AutoModel
                import torch
                from config import get_device, DEFAULT_MODEL
                
                def run_quantization_examples():
                    """Run quantization examples."""
                    
                    print(f"Loading model: {DEFAULT_MODEL}")
                    device = get_device()
                    print(f"Using device: {device}")
                    
                    # Example implementation
                    tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL)
                    model = AutoModel.from_pretrained(DEFAULT_MODEL)
                    
                    # Example text
                    text = "Hugging Face Transformers make NLP accessible to everyone!"
                    
                    # Tokenize
                    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
                    
                    print(f"\nInput text: {text}")
                    print(f"Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].tolist())}")
                    print(f"Token IDs: {inputs['input_ids'][0].tolist()}")
                    
                    # Get model outputs
                    with torch.no_grad():
                        outputs = model(**inputs)
                    
                    print(f"\nModel output shape: {outputs.last_hidden_state.shape}")
                    print("Example completed successfully!")
                    
                if __name__ == "__main__":
                    print("=== Quantization Examples ===\n")
                    run_quantization_examples()
              metadata:
                extension: .py
                size_bytes: 1200
                language: python
            tests/test_examples.py:
              content: |
                """Unit tests for Chapter 09 examples."""
                
                import pytest
                import sys
                from pathlib import Path
                
                # Add src to path
                sys.path.append(str(Path(__file__).parent.parent / "src"))
                
                from config import get_device
                from quantization import run_quantization_examples
                
                def test_device_detection():
                    """Test that device detection works."""
                    device = get_device()
                    assert device in ["cpu", "cuda", "mps"]
                    
                def test_quantization_runs():
                    """Test that quantization examples run without errors."""
                    # This is a basic smoke test
                    try:
                        run_quantization_examples()
                    except Exception as e:
                        pytest.fail(f"quantization examples failed: {e}")
                        
                def test_imports():
                    """Test that all required modules can be imported."""
                    import transformers
                    import torch
                    import numpy
                    import pandas
                    
                    assert transformers.__version__
                    assert torch.__version__
              metadata:
                extension: .py
                size_bytes: 800
                language: python
      metadata:
        extension: .yaml
        size_bytes: 13586
        language: yaml
    Taskfile.yml:
      content: |
        version: '3'

        vars:
          PYTHON_VERSION: 3.12.9

        tasks:
          default:
            desc: "Show available tasks"
            cmds:
              - task --list

          setup:
            desc: "Set up the Python environment and install dependencies"
            cmds:
              - pyenv install -s {{.PYTHON_VERSION}}
              - pyenv local {{.PYTHON_VERSION}}
              - poetry install
              - poetry config virtualenvs.in-project true
              - 'echo "Setup complete! Activate with: source .venv/bin/activate"'

          run:
            desc: "Run all examples"
            cmds:
              - poetry run python src/main.py

          run-quantization:
            desc: "Run quantization examples"
            cmds:
              - poetry run python src/quantization.py

          run-pruning:
            desc: "Run pruning examples"
            cmds:
              - poetry run python src/pruning.py

          run-onnx-export:
            desc: "Run onnx export examples"
            cmds:
              - poetry run python src/onnx_export.py

          test:
            desc: "Run all tests"
            cmds:
              - poetry run pytest tests/ -v

          format:
            desc: "Format code with Black and Ruff"
            cmds:
              - poetry run black src/ tests/
              - poetry run ruff check --fix src/ tests/

          clean:
            desc: "Clean up generated files"
            cmds:
              - find . -type d -name "__pycache__" -exec rm -rf {} +
              - find . -type f -name "*.pyc" -delete
              - rm -rf .pytest_cache
              - rm -rf .ruff_cache
              - rm -rf .mypy_cache
      metadata:
        extension: .yml
        size_bytes: 1335
        language: yaml
    CLAUDE.md:
      content: |-
        # CLAUDE.md

        This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

        ## Project Overview

        This is a Python educational codebase for Chapter 9 of a Hugging Face Transformers book, focusing on "Model Optimization and Deployment". The project demonstrates various optimization techniques including quantization, pruning, and ONNX export.

        ## Essential Commands

        ### Setup and Development
        ```bash
        # Initial setup (installs Python 3.12.9, dependencies, creates venv)
        task setup
        source .venv/bin/activate

        # Run all examples
        task run

        # Run specific optimization examples
        task run-quantization
        task run-pruning
        task run-onnx-export

        # Run tests
        task test

        # Format code (Black + Ruff)
        task format

        # Clean generated files
        task clean
        ```

        ### Direct Execution
        ```bash
        # Run with Poetry (if not using task)
        poetry run python src/main.py
        poetry run pytest tests/ -v
        ```

        ## Architecture Overview

        ### Core Design Pattern
        The project follows a modular architecture where each optimization technique is isolated in its own module. All modules follow a consistent pattern:

        1. **Configuration**: Centralized in `src/config.py` using environment variables
        2. **Entry Point**: `src/main.py` orchestrates all examples sequentially
        3. **Modules**: Each technique (quantization, pruning, etc.) in separate files
        4. **Testing**: Smoke tests in `tests/` ensure basic functionality

        ### Key Architectural Components

        - **Device Management**: Automatic detection of CUDA/MPS/CPU in `config.get_device()`
        - **Path Management**: All paths relative to `PROJECT_ROOT` in config.py
        - **Environment Config**: `.env` file for API keys and model settings
        - **Directory Creation**: Automatic creation of `data/` and `models/` directories

        ### Module Interaction Flow
        ```
        main.py → Individual modules (quantization.py, pruning.py, etc.)
           ↓              ↓
        print_section()  config.py (shared configuration)
                           ↓
                       External APIs (OpenAI, Anthropic, HuggingFace)
        ```

        ### Implementation Status
        Currently implemented:
        - `config.py`: Complete configuration management
        - `main.py`: Orchestration with placeholder imports
        - `quantization.py`: Basic quantization examples

        Missing (referenced but not created):
        - `pruning.py`: Model pruning implementation
        - `onnx_export.py`: ONNX export functionality
        - `performance_benchmarking.py`: Performance measurement
        - `utils.py`: Utility functions

        ## Development Patterns

        ### Adding New Optimization Techniques
        1. Create new module in `src/` (e.g., `new_technique.py`)
        2. Implement `run_new_technique_examples()` function
        3. Import and call in `main.py`
        4. Add corresponding task in `Taskfile.yml`
        5. Create tests in `tests/test_examples.py`

        ### Testing Approach
        - Use pytest for all tests
        - Smoke tests verify module imports and basic execution
        - Test device detection and configuration loading
        - Run with `task test` or `poetry run pytest tests/ -v`

        ### Code Quality Standards
        - Black formatting (line length 88)
        - Ruff linting with rules: E, F, I, N, UP, B, C4, SIM
        - Python 3.12 type hints where applicable
        - Clear console output for educational purposes

        ## Environment Configuration

        Required environment variables (see `.env.example`):
        - `DEFAULT_MODEL`: Model to use (default: bert-base-uncased)
        - `BATCH_SIZE`: Batch size for processing (default: 8)
        - `MAX_LENGTH`: Maximum sequence length (default: 512)
        - `OPENAI_API_KEY`: OpenAI API access
        - `ANTHROPIC_API_KEY`: Anthropic API access
        - `HUGGINGFACE_API_KEY`: HuggingFace API access

        ## Key Dependencies

        - **transformers**: Core Hugging Face library
        - **onnx & onnxruntime**: ONNX export and inference
        - **optimum**: HuggingFace optimization library
        - **torch**: PyTorch for model operations
        - **python-dotenv**: Environment variable management

        ## Common Development Tasks

        ### Running a single test
        ```bash
        poetry run pytest tests/test_examples.py::test_quantization_examples -v
        ```

        ### Checking GPU/MPS availability
        ```python
        from src.config import get_device
        print(get_device())  # Returns 'cuda', 'mps', or 'cpu'
        ```

        ### Adding new API keys
        1. Add to `.env.example` as template
        2. Add to `config.py` with `os.getenv()`
        3. Document in README.md

        ### Debugging imports
        Check that all modules in `main.py` exist in `src/` directory. Missing modules will cause ImportError.
      metadata:
        extension: .md
        size_bytes: 4332
        language: markdown
    .claude/settings.local.json:
      content: |-
        {
          "permissions": {
            "allow": [
              "Bash(ls:*)"
            ],
            "deny": []
          }
        }
      metadata:
        extension: .json
        size_bytes: 82
        language: json
    docs/article9.md:
      content: |-
        # From Keywords to Neural Understanding: The Transformer Revolution in Search

        Modern information retrieval has undergone a profound transformation. Where once we struggled with keyword limitations and boolean operators, today's semantic search unlocks the true meaning behind our questions. This chapter explores how transformer models have revolutionized search by bridging the gap between what users ask and what they truly seek.

        We'll journey through:

        - The **fundamental shift** from lexical matching to semantic understanding
        - How **transformer architectures** create rich, contextual embeddings that capture meaning
        - **Vector databases** that make these embeddings searchable at scale
        - **Real-world applications** across customer support, knowledge management, and legal discovery
        - The **latest advancements** including RAG integration and specialized domain models

        By understanding both the theory and practical implementation of transformer-based search, you'll gain the tools to build systems that truly comprehend user intent—not just match strings. Let's explore how these neural networks have fundamentally changed what's possible in information retrieval.

        # Semantic Search and Information Retrieval with Transformers - Article 9

        ```mermaid
        mindmap
          root((Semantic Search & Information Retrieval))
            From Keywords to Understanding
              Keyword Limitations
              Context Recognition
              Meaning-Based Retrieval
            Transformer Embeddings
              Dense Vectors
              Sentence Transformers
              Multilingual Models
              Domain Adaptation
            Vector Databases & FAISS
              Scalable Storage
              Fast Similarity Search
              Production Integration
              Hybrid Approaches
            Business Applications
              Customer Support
              Knowledge Management
              Legal Discovery
              Enterprise Search
            Modern Features
              RAG Integration
              Long-Context Support
              Cloud APIs
              MTEB Benchmarks

        ```

        **Step-by-Step Explanation:**

        - Root node focuses on **Semantic Search & Information Retrieval**
        - Branch shows transition **From Keywords to Understanding** with limitations and benefits
        - Branch covers **Transformer Embeddings** including models and adaptations
        - Branch details **Vector Databases & FAISS** for scalable implementation
        - Branch highlights **Business Applications** across industries
        - Branch includes **Modern Features** like RAG and benchmarking

        ## Introduction: From Keyword Search to True Understanding

        Ever chased a critical document that you *know* exists, but can't find because you're using the wrong words? That's keyword search failing you. Today's transformer models transform this frustration into fluid discovery. They understand **meaning**, not just matching letters.

        Picture searching a massive warehouse for a red umbrella. Keyword search gives you a flashlight that only illuminates boxes labeled "red." Miss the one marked "crimson parasol"? Too bad. You'll walk right past it.

        Now imagine a brilliant assistant who grasps that crimson equals red, and parasol means umbrella. They understand your **intent**, not just your words. That's semantic search—it discovers meaning, not just matches.

        **Semantic search** harnesses transformer models—deep learning architectures that capture relationships between words in context. Instead of literal matching, transformers encode the essence behind text. "Refund policy" and "money back guarantee" become neighbors in meaning space, even sharing zero words.

        Let's witness this transformation with Python. We'll contrast keyword and semantic search using the latest `sentence-transformers` library and production-ready patterns.

        ### Setting Up Your Environment with Python 3.12.9

        ```bash
        # Using pyenv (recommended for Python version management)
        pyenv install 3.12.9
        pyenv local 3.12.9

        # Verify Python version
        python --version  # Should show Python 3.12.9

        # Install required packages with poetry
        poetry new semantic-search-project
        cd semantic-search-project
        poetry env use 3.12.9
        poetry add sentence-transformers numpy faiss-cpu

        # Or use mini-conda
        conda create -n semantic-search python=3.12.9
        conda activate semantic-search
        pip install sentence-transformers numpy faiss-cpu

        # Or use pip with pyenv
        pyenv install 3.12.9
        pyenv local 3.12.9
        pip install sentence-transformers numpy faiss-cpu

        ```

        ### Keyword Search vs. Semantic Search: A Modern Comparison

        ```python
        # Example FAQ documents
        faqs = [
            "How can I reset my password?",
            "What are the steps for account recovery?",
            "How do I request a refund?",
            "Information about our privacy policy."
        ]

        # User query
        query = "I forgot my login credentials"

        # --- Keyword Search ---
        # Find FAQs containing any keyword from the query (exact match)
        keywords = set(query.lower().split())
        keyword_matches = [faq for faq in faqs if keywords & set(faq.lower().split())]
        print("Keyword Search Results:", keyword_matches)

        # --- Semantic Search ---
        # Use a transformer model to embed both the FAQs and the query
        from sentence_transformers import SentenceTransformer, util
        import numpy as np

        model = SentenceTransformer('all-MiniLM-L6-v2')  # Current, lightweight model

        faq_embeddings = model.encode(faqs, convert_to_numpy=True)  # Embeddings as vectors
        query_embedding = model.encode([query], convert_to_numpy=True)[0]

        # Cosine similarity measures semantic closeness between query and each FAQ
        cosine_scores = util.cos_sim(query_embedding, faq_embeddings)[0].cpu().numpy()

        # Rank FAQs by similarity (highest score first)
        top_idx = np.argsort(-cosine_scores)
        semantic_matches = [faqs[i] for i in top_idx[:2]]
        print("Semantic Search Results:", semantic_matches)

        # Note: For production-scale search, use a vector database (see below) for efficiency.

        ```

        **Step-by-Step Explanation:**

        1. **Define FAQs and Query**: Create sample documents and a user question that doesn't match keywords exactly
        2. **Keyword Search**: Split query into words, find FAQs sharing any word—misses relevant answers when wording differs
        3. **Load Transformer Model**: Initialize sentence transformer that creates meaning-rich embeddings
        4. **Generate Embeddings**: Convert FAQs and query into dense vectors capturing semantic essence
        5. **Calculate Similarity**: Use cosine similarity to measure meaning closeness between vectors
        6. **Rank Results**: Sort FAQs by similarity score—most relevant surfaces first

        Notice how keyword search returns nothing (no shared words), while semantic search correctly identifies password/account recovery FAQs as relevant. That's the power of understanding **meaning**.

        Why does this transformation matter for business?

        - **Customer Support**: Users rarely phrase questions matching your documentation. Semantic search bridges this gap
        - **Enterprise Knowledge**: Employees discover procedures using their own terminology
        - **Legal Compliance**: Lawyers surface relevant precedents by meaning, not exact phrasing

        Transformers fuel this leap. They absorb context and nuance from massive datasets, enabling search that transcends surface matching.

        **Key takeaway**: Semantic search, powered by transformers, unlocks genuine language understanding. This shift proves vital for building smarter, more intuitive search across domains.

        🔎 **Production Note:** Real-world semantic search stores embeddings in vector databases (FAISS, Milvus, Weaviate) for efficient scaling to millions of documents. We'll explore this shortly.

        🌐 **Looking Ahead:** Recent advances include retrieval-augmented generation (RAG), Mixture of Experts architectures, and multimodal search combining text, images, and video. Later articles dive deep into these cutting-edge capabilities.

        ## Introduction to Semantic Search

        Search drives how we navigate information—from company wikis to legal archives. Traditional engines focus on exact matches, missing true intent. Semantic search revolutionizes this by understanding **meaning and context**, leveraging transformer embeddings.

        You'll master how semantic search surpasses keyword matching, why transformer embeddings excel at capturing meaning, and which metrics prove search quality. We'll introduce production practices—vector databases, hybrid search, modern evaluation tools—through practical examples.

        ```mermaid
        flowchart TB
            subgraph Traditional Search
                Query1[User Query] -->|Exact Match| Keywords[Keyword Engine]
                Keywords --> Results1[Limited Results]
            end

            subgraph Semantic Search
                Query2[User Query] -->|Embedding| Transformer[Transformer Model]
                Docs[Documents] -->|Embedding| Transformer
                Transformer --> Vectors[Vector Space]
                Vectors -->|Similarity| Results2[Relevant Results]
            end

            Results1 -->|"Misses Intent"| Frustrated[Frustrated User]
            Results2 -->|"Understands Meaning"| Happy[Happy User]

            classDef default fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            class Query1,Keywords,Results1,Query2,Transformer,Docs,Vectors,Results2,Frustrated,Happy default

        ```

        **Step-by-Step Explanation:**

        - **Traditional Search** path shows query going through keyword engine to limited results
        - User becomes frustrated when intent is missed
        - **Semantic Search** path shows both query and documents becoming embeddings
        - Transformer creates vectors that capture meaning
        - Similarity matching produces relevant results
        - User achieves satisfaction through understood intent

        ### Keyword vs. Semantic Search

        Picture searching for "resetting your password." Keyword search only finds documents containing "reset" and "password"—missing "Account recovery steps" despite being your answer.

        Think of keyword search as a rigid librarian who only fetches books with your exact phrase. Fast, but inflexible. Synonyms or paraphrasing derail it completely.

        Semantic search resembles an insightful librarian who grasps your meaning. It connects "resetting password" with "account recovery" by matching **intent**, not letters. This magic happens through embeddings—numeric vectors representing text meaning, generated by transformers.

        Embeddings capture context and word relationships. We compare meanings using **cosine similarity**—a mathematical measure of vector direction closeness. Nearby vectors share similar meaning.

        ### Comparing Keyword and Semantic Search Results

        ```python
        # Ensure Python 3.12.9 environment
        import sys
        print(f"Python version: {sys.version}")  # Verify 3.12.9

        # Example query and candidate documents
        query = "How do I reset my password?"
        documents = [
            "Account recovery steps",
            "Password reset instructions",
            "Update your profile information"
        ]

        # Keyword search: match if 'password' is present
        keyword_matches = [doc for doc in documents if "password" in doc.lower()]
        print("Keyword Search Results:", keyword_matches)

        # Semantic search: use a sentence transformer for meaning
        from sentence_transformers import SentenceTransformer, util
        # You can also try newer embedding models, such as 'BAAI/bge-base-en-v1.5' or 'intfloat/e5-base-v2'
        model = SentenceTransformer('all-MiniLM-L6-v2')
        query_embedding = model.encode(query)
        doc_embeddings = model.encode(documents)
        scores = util.cos_sim(query_embedding, doc_embeddings)[0]
        top_idx = scores.argmax().item()
        print("Semantic Search Top Result:", documents[top_idx])

        ```

        **Step-by-Step Explanation:**

        1. **Verify Python Version**: Ensure we're using Python 3.12.9 for consistency
        2. **Define Query and Documents**: Create search scenario with varied phrasings
        3. **Keyword Search**: Only finds "Password reset instructions"—ignores relevant "Account recovery"
        4. **Load Transformer**: Initialize model that creates semantic embeddings
        5. **Generate Embeddings**: Convert text into meaning vectors
        6. **Calculate Similarity**: Find document with closest meaning to query
        7. **Surface Best Match**: Semantic search correctly identifies most relevant document

        Try modifying the query to "forgot login details"—watch how results shift. This demonstrates semantic search uncovering relevance that keyword search misses entirely.

        Production systems scale by storing embeddings in vector databases (FAISS, Milvus, Pinecone, Weaviate), enabling lightning-fast similarity search across millions of documents.

        Modern engines combine keyword and semantic search (hybrid search) maximizing precision and recall. Reranking models (cross-encoders, LLMs) further refine top results.

        **Key Takeaway:** Semantic search matches meaning, not just words. Transformer embeddings plus vector databases deliver helpful, scalable, accurate results.

        ### Business Applications of Semantic Search

        Semantic search isn't merely technical evolution—it's competitive advantage. Real-world impact:

        1. **Enterprise Knowledge Bases:** Employees use varied terminology. Semantic search bridges vocabulary gaps, surfacing answers regardless of phrasing.
        2. **Customer Support Automation:** Intent-aware chatbots understand "How can I get my money back?" matches refund policies—without the word "refund."
        3. **Legal and Compliance Discovery:** Legal teams find relevant precedents through meaning, not just keywords—saving hours, reducing risk.

        Production deployments leverage scalable vector databases for efficient embedding retrieval, often combining keyword and semantic methods for peak accuracy.

        **Summary:** Semantic search reduces friction, boosts satisfaction, makes knowledge work efficient—especially with cutting-edge embedding models and infrastructure.

        ### Vector Database Comparison Table

        | Database | Type | Strengths | Best For | Scaling |
        | --- | --- | --- | --- | --- |
        | **FAISS** | Open-source library | Fast, flexible indices | Research, custom deployments | Manual sharding |
        | **Pinecone** | Managed cloud | Zero-ops, auto-scaling | SaaS applications | Automatic |
        | **Weaviate** | Open-source + cloud | GraphQL API, hybrid search | Enterprise search | Horizontal |
        | **Milvus** | Open-source + cloud | GPU support, high performance | Large-scale ML | Distributed |
        | **Qdrant** | Open-source + cloud | Rust-based, filtering | Real-time applications | Cloud-native |
        | **Chroma** | Embedded/cloud | Simple API, developer-friendly | Prototyping, RAG | In-memory to cloud |

        ### Measuring Search Quality: Metrics that Matter

        Building search is half the battle. Proving it works requires clear metrics:

        1. **Precision:** What fraction of results are relevant? High precision minimizes irrelevant noise.
        2. **Recall:** What fraction of relevant documents appeared? High recall ensures nothing important is missed.
        3. **F1 Score:** Harmonizes precision and recall into one balanced metric.
        4. **Mean Reciprocal Rank (MRR):** How high does the first relevant result appear? Top placement delights users.
        5. **Normalized Discounted Cumulative Gain (NDCG):** Evaluates entire ranking, rewarding relevant results near the top—crucial for result lists.

        ### Calculating Precision and Recall for Search Results

        ```python
        # Example: Evaluating a search system
        retrieved = {"doc1", "doc2", "doc5"}  # IDs of documents returned by search
        relevant = {"doc2", "doc3", "doc5"}   # IDs of truly relevant documents

        precision = len(retrieved & relevant) / len(retrieved)
        recall = len(retrieved & relevant) / len(relevant)
        print(f"Precision: {precision:.2f}")
        print(f"Recall: {recall:.2f}")
        # Output: Precision: 0.67, Recall: 0.67

        ```

        **Step-by-Step Explanation:**

        1. **Define Sets**: Retrieved documents vs. truly relevant documents
        2. **Calculate Intersection**: Find documents that are both retrieved and relevant
        3. **Compute Precision**: Fraction of retrieved that are relevant
        4. **Compute Recall**: Fraction of relevant that were retrieved

        Practice requires averaging metrics across many queries. For ranking metrics (MRR, NDCG), libraries like Hugging Face's `evaluate`, `scikit-learn`, or `pytrec_eval` automate calculations.

        Choose metrics matching your goals. Customer support needs high recall—never miss helpful articles. Legal search demands high precision—avoid irrelevant results.

        **Key Point:** Metrics transform search quality from guesswork into measurable outcomes, guiding continuous improvement.

        ## Embedding Text for Search Applications

        Imagine organizing a vast library by **meaning**, not titles. That's semantic search's challenge—embeddings solve it. You'll learn to transform text into vectors, store them efficiently, adapt for languages and specialized domains. We'll highlight cutting-edge models and databases for future-proof, production-ready solutions.

        ```mermaid
        stateDiagram-v2
            [*] --> RawText
            RawText --> Tokenization: Preprocess
            Tokenization --> TransformerModel: Encode
            TransformerModel --> Embeddings: Generate Vectors
            Embeddings --> VectorDatabase: Store
            VectorDatabase --> SearchReady: Index
            SearchReady --> [*]

            RawText --> MultilingualCheck: Check Language
            MultilingualCheck --> MultilingualModel: Non-English
            MultilingualModel --> Embeddings

            RawText --> DomainCheck: Check Domain
            DomainCheck --> DomainModel: Specialized
            DomainModel --> Embeddings

            style RawText fill:#bbdefb,stroke:#1976d2,stroke-width:1px,color:#333333
            style Embeddings fill:#c8e6c9,stroke:#43a047,stroke-width:1px,color:#333333
            style SearchReady fill:#c8e6c9,stroke:#43a047,stroke-width:1px,color:#333333

        ```

        **Step-by-Step Explanation:**

        - Start with **RawText** that needs embedding
        - Text goes through **Tokenization** preprocessing
        - **TransformerModel** encodes tokens into **Embeddings**
        - Embeddings stored in **VectorDatabase** and indexed
        - System becomes **SearchReady**
        - Parallel paths handle **Multilingual** and **Domain-specific** content

        Coverage includes:

        1. What embeddings are and their importance
        2. Creating them with sentence transformers and modern APIs
        3. Storing and managing embeddings at scale
        4. Multilingual and domain-specific strategies
        5. Selecting optimal models and databases

        ### What Are Embeddings—and Why Do They Matter?

        An embedding is a numerical fingerprint—a vector—capturing text meaning. Picture it as DNA for sentences. Similar meanings yield similar embeddings, clustering in vector space.

        Consider:

        - "How do I reset my password?"
        - "What are the steps to recover my account?"

        Keyword search sees these as unrelated. Embeddings recognize their semantic kinship, enabling intelligent matching.

        Practical impact? Embeddings power:

        - Customer question routing to relevant help articles—despite different wording
        - Support ticket clustering by issue type
        - Cross-language search focusing on meaning
        - Retrieval-augmented generation (RAG) combining search with LLMs for reasoning

        **Bottom line**: Embeddings translate language into comparable meanings, not just matched strings.

        ### Generating Embeddings with Sentence Transformers and Modern APIs

        While classic transformers like BERT understand language, they weren't designed for sentence comparison. **Sentence transformers** train specifically to cluster similar meanings—perfect for search. Recent APIs (Google Gemini, OpenAI, Cohere) raise the bar for multilingual and domain performance.

        The `sentence-transformers` library remains essential for local workflows. For English, `'all-MiniLM-L6-v2'` delivers speed and accuracy. Production deployments often use cloud APIs or cutting-edge open models. **Pro tip:** Check the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for latest benchmarks.

        ### Generating Embeddings with Sentence Transformers

        ```python
        from sentence_transformers import SentenceTransformer

        # Load a pre-trained sentence transformer model
        model = SentenceTransformer('all-MiniLM-L6-v2')

        # Example sentences to embed
        sentences = [
            "How do I reset my password?",
            "Account recovery steps"
        ]

        # Generate embeddings (returns a NumPy array of floats)
        embeddings = model.encode(sentences)

        print(embeddings.shape)  # Output: (2, 384)

        ```

        **Step-by-Step Explanation:**

        1. **Import Library**: Load sentence-transformers for embedding generation
        2. **Load Model**: Initialize pre-trained model from Hugging Face
        3. **Define Sentences**: Create text samples to embed
        4. **Generate Embeddings**: Convert sentences to 384-dimensional vectors
        5. **Verify Shape**: Confirm output dimensions match expectations

        Similar sentences produce similar vectors, enabling meaningful comparison. For other languages or industries, choose models trained on relevant data. Modern APIs support longer sequences—essential for document search and RAG.

        For state-of-the-art performance, explore Google Gemini Embedding, OpenAI's latest APIs, or top MTEB models supporting extended inputs, superior accuracy, and broader language coverage.

        ### Modern Embedding API Example with OpenAI

        ```python
        # Example using OpenAI's embedding API (requires API key)
        import openai
        import numpy as np

        # Set your OpenAI API key
        openai.api_key = "your-api-key-here"

        def get_openai_embeddings(texts):
            """Generate embeddings using OpenAI's latest text-embedding model"""
            response = openai.Embedding.create(
                model="text-embedding-ada-002",  # Or newer models as available
                input=texts
            )
            embeddings = [item['embedding'] for item in response['data']]
            return np.array(embeddings)

        # Example usage
        texts = ["How do I reset my password?", "Account recovery steps"]
        openai_embeddings = get_openai_embeddings(texts)
        print(f"OpenAI embeddings shape: {openai_embeddings.shape}")  # (2, 1536)

        # Compare with sentence-transformers for cost/performance trade-offs
        # OpenAI: Higher accuracy, API costs, larger dimensions
        # Sentence-transformers: Free, local, smaller dimensions

        ```

        **Step-by-Step Explanation:**

        1. **Import Libraries**: Load OpenAI client and NumPy
        2. **Set API Key**: Configure authentication for OpenAI
        3. **Define Function**: Create reusable embedding generator
        4. **Call API**: Request embeddings from OpenAI's model
        5. **Extract Vectors**: Parse response into NumPy array
        6. **Compare Options**: Understand trade-offs between APIs and local models

        ### Batch Processing and Storing Embeddings

        Real systems handle thousands of documents—product catalogs, support tickets, articles. Batch processing accelerates embedding creation. You'll need persistent storage for search. **Vector databases** are now industry standard for production.

        ### Batch Embedding and Saving to Disk

        ```python
        import numpy as np

        # Example: List of document texts
        documents = ["Doc 1 text", "Doc 2 text", "Doc 3 text"]

        # Batch encode documents (adjust batch_size for your hardware)
        doc_embeddings = model.encode(documents, batch_size=32, show_progress_bar=True)

        # Save embeddings as a NumPy array for later use
        np.save('doc_embeddings.npy', doc_embeddings)

        ```

        **Step-by-Step Explanation:**

        1. **Prepare Documents**: Create list of texts to embed
        2. **Batch Encode**: Process multiple documents efficiently
        3. **Show Progress**: Track encoding progress for large batches
        4. **Save Embeddings**: Store vectors in NumPy format for fast loading

        ⚡ **Pro Tip:** Always maintain document ID mapping. Track which embedding corresponds to which document—critical for retrieving correct text later.

        For production search, choose vector databases like FAISS, Qdrant, Milvus, or Chroma—optimized for embedding storage and similarity search at scale.

        **Summary**: Batch processing and organized storage enable scalable semantic search. Select databases matching your performance and deployment needs.

        ### Multilingual and Domain-Specific Embeddings

        Global businesses operate across languages and specialized fields. How do embeddings capture meaning across these dimensions? Recent advances dramatically improve multilingual and domain performance.

        **Multilingual embeddings:**
        Models like `'paraphrase-multilingual-MiniLM-L12-v2'` support 50+ languages. Google Gemini and OpenAI embeddings offer even broader coverage. Similar meanings cluster together regardless of language.

        ### Generating Multilingual Embeddings

        ```python
        multi_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

        sentences = [
            "How do I reset my password?",        # English
            "¿Cómo puedo restablecer mi contraseña?",  # Spanish
            "Comment réinitialiser mon mot de passe?"  # French
        ]

        multi_embeddings = multi_model.encode(sentences)
        print(multi_embeddings.shape)  # Output: (3, 384)

        ```

        **Step-by-Step Explanation:**

        1. **Load Multilingual Model**: Initialize model supporting multiple languages
        2. **Define Multilingual Sentences**: Same meaning in three languages
        3. **Generate Embeddings**: Create vectors capturing cross-language meaning
        4. **Verify Consistency**: All produce same-dimension embeddings

        These three sentences share meaning—multilingual models ensure their embeddings cluster together, enabling seamless cross-language search.

        **Domain-specific embeddings:**
        Technical fields need specialized understanding. Options:

        - Search Hugging Face for domain models (legal, biomedical, financial)
        - Fine-tune base models on your data (see Article 10)
        - Use APIs with strong domain-specific MTEB performance

        **Long-context support:**
        Modern embeddings (Gemini, OpenAI) handle extended sequences—ideal for document search and RAG applications requiring large context.

        **In summary:**

        - Deploy multilingual models for global reach
        - Adapt models for specialized domains
        - Prefer long-context support for documents
        - Benchmark using MTEB for optimal selection

        Right model choice boosts search quality and user satisfaction dramatically.

        ### Key Takeaways and Next Steps

        Embeddings transform text into semantic vectors, enabling meaning-based search. Sentence transformers and modern APIs simplify generation and storage at scale. Match models to your language, domain, and context needs using MTEB benchmarks.

        **Try this**: Embed three sentences from your domain. Compare their vectors—notice meaning clusters. For production, experiment with Qdrant, Milvus, or Chroma for managed search.

        **Next**: Discover how embeddings power real search with vector databases and FAISS (this chapter). Explore fine-tuning for custom needs (Article 10). For advanced applications, embeddings enable RAG systems combining search with LLM reasoning.

        ## Vector Databases and FAISS Integration

        Semantic search retrieves **meaning** across massive datasets. FAISS (Facebook AI Similarity Search) delivers high-performance, open-source vector search and clustering. Scale from thousands to billions of records while maintaining low latency. You'll install FAISS, build indices, and apply production best practices for scaling. We'll cover recent features, quantization, and integration with distributed search.

        ```mermaid
        classDiagram
            class FAISS {
                +IndexFlatL2 exactSearch
                +IndexIVFFlat approximateSearch
                +IndexHNSWFlat graphSearch
                +IndexPQ quantizedSearch
                +add(embeddings)
                +search(query, k)
                +train(data)
            }

            class VectorDatabase {
                +store(embeddings)
                +query(vector, k)
                +update(id, vector)
                +delete(id)
            }

            class ProductionSystem {
                +SentenceTransformer encoder
                +FAISS index
                +DocumentStore metadata
                +hybridSearch()
                +rerank()
            }

            FAISS <|-- IndexFlatL2
            FAISS <|-- IndexIVFFlat
            FAISS <|-- IndexHNSWFlat
            FAISS <|-- IndexPQ

            VectorDatabase <|-- Pinecone
            VectorDatabase <|-- Weaviate
            VectorDatabase <|-- Milvus

            ProductionSystem --> FAISS
            ProductionSystem --> VectorDatabase

        ```

        **Step-by-Step Explanation:**

        - **FAISS** base class provides vector search functionality
        - Subclasses offer different index types for various use cases
        - **VectorDatabase** interface implemented by managed solutions
        - **ProductionSystem** integrates FAISS with transformers and metadata
        - Shows relationships between components in real deployments

        ### Setting Up FAISS for Scalable Similarity Search

        FAISS excels at similarity search over high-dimensional vectors from transformers. It supports exact search (true nearest neighbors) and approximate search (faster with minor accuracy trade-offs). Version 1.7.2+ adds improved GPU support and enhanced algorithms.

        Install FAISS choosing CPU or GPU versions based on your hardware. Ensure Python 3.7+ and NumPy 1.18+ compatibility.

        ### Installing and Importing FAISS (v1.7.2+)

        ```python
        # Using Python 3.12.9 environment
        # Install FAISS for CPU
        # pip install faiss-cpu
        # Or for GPU (CUDA)
        # pip install faiss-gpu

        # Or with poetry (recommended)
        # poetry add faiss-cpu

        # Or with conda
        # conda install -c pytorch faiss-cpu

        import faiss
        import numpy as np

        print(f"FAISS version: {faiss.__version__}")  # Should be 1.7.2 or higher
        print(f"Python version: {sys.version}")  # Verify 3.12.9

        ```

        FAISS requires embeddings as NumPy arrays with dtype 'float32'. Convert other types before indexing to prevent errors.

        ### Ensuring Embedding Data Type Compatibility

        ```python
        embeddings = np.array(embeddings, dtype=np.float32)  # Always use float32

        ```

        Create your first FAISS index—a data structure enabling fast similarity search. For small-medium datasets, exact search works perfectly.

        ### Creating a Simple FAISS Index (Exact Search)

        ```python
        # Assume 'embeddings' is a (num_vectors, dimension) float32 array
        dimension = embeddings.shape[1]  # Embedding size
        index = faiss.IndexFlatL2(dimension)  # Exact search with L2 distance
        index.add(embeddings)  # Add embeddings to index

        ```

        **Step-by-Step Explanation:**

        1. **Get Dimension**: Extract embedding vector length (e.g., 384)
        2. **Create Index**: Initialize exact L2 distance index
        3. **Add Data**: Load embeddings for fast searching

        For larger datasets, explore approximate or compressed indices (covered in scaling section).

        ### Indexing, Querying, and Updating Search Indices

        With your index built, search for similar items by embedding queries and using FAISS's search API. Always use the same model and preprocessing for queries as your indexed data.

        Process:

        1. Generate query embedding (same model as documents)
        2. Search FAISS index for nearest neighbors
        3. Map results to original data

        ### Performing a Search Query with Error Handling

        ```python
        try:
            query_text = "How can I get my money back?"
            query_embedding = model.encode([query_text]).astype(np.float32)
            # Ensure shape is (1, dimension)
            if query_embedding.ndim == 1:
                query_embedding = query_embedding.reshape(1, -1)
            distances, indices = index.search(query_embedding, k=3)  # Top-3 matches
            print("Top 3 matching document indices:", indices[0])
            print("Corresponding distances:", distances[0])
        except Exception as e:
            print(f"An error occurred during FAISS search: {e}")

        ```

        **Step-by-Step Explanation:**

        1. **Define Query**: User's search question
        2. **Generate Embedding**: Convert query to vector using same model
        3. **Reshape if Needed**: Ensure proper dimensions for FAISS
        4. **Search Index**: Find top-k most similar embeddings
        5. **Handle Errors**: Gracefully manage potential issues
        6. **Display Results**: Show matching indices and distances

        Lower distance indicates higher similarity for L2 distance metric.

        Adding new documents: generate embeddings and use `index.add()`. Most indices support fast, safe addition. Note: Training-required indices (IVF) need training before adding data.

        Deletion/updates are complex. Version 1.7.2 supports limited deletion (IndexIVFFlat with IDMap). Best practice: periodically rebuild indices for frequent changes.

        Always save indices after major updates for reliability and fast recovery.

        ### Saving and Loading a FAISS Index (v1.7.2+)

        ```python
        # Save the index
        faiss.write_index(index, "my_faiss.index")

        # Load it later
        index = faiss.read_index("my_faiss.index")

        ```

        Production tip: Version indices and store in durable locations (object storage, distributed filesystems) for disaster recovery.

        ### Scaling Semantic Search

        Growing datasets make exact search (`IndexFlatL2`) slow or memory-intensive. FAISS offers advanced scaling strategies: approximate nearest neighbor (ANN) search, quantization, GPU acceleration. Version 1.7.2+ adds LSQ quantization on GPU and improved clustering.

        1. **Approximate Nearest Neighbor (ANN) Search**

        ANN algorithms dramatically accelerate search by returning close (not always exact) matches. FAISS options:

        - `IndexIVFFlat`: Inverted File with Flat quantization (requires training)
        - `IndexHNSWFlat`: Hierarchical Navigable Small World graphs (no training, fast)
        - `IndexLSQ`: Locally Sensitive Quantization (GPU support in v1.7.2)
        - `IndexPQ`: Product Quantization for compression

        Choose based on dataset size, accuracy needs, and hardware.

        ### Building an Approximate Search Index with FAISS (IVF)

        ```python
        # Set up an IVF index for large-scale search
        nlist = 100  # Number of clusters (tune for your data)
        quantizer = faiss.IndexFlatL2(dimension)  # Used for cluster assignment
        index_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)

        # Must train the index before adding data
        index_ivf.train(embeddings)
        index_ivf.add(embeddings)

        # Enable parallel search for faster queries (optional, multi-core CPUs)
        faiss.omp_set_num_threads(4)

        ```

        **Step-by-Step Explanation:**

        1. **Define Clusters**: Set number of partitions for vector space
        2. **Create Quantizer**: Initialize cluster assignment mechanism
        3. **Build IVF Index**: Combine quantizer with inverted file structure
        4. **Train Index**: Learn cluster centers from data
        5. **Add Data**: Insert embeddings into trained index
        6. **Enable Parallelism**: Use multiple CPU cores for faster search

        `IndexIVFFlat` partitions space into clusters for faster search. Training required before data addition. `nlist` balances speed vs. recall.

        1. **Quantization and Compression**

        For massive datasets, quantization reduces memory with minimal accuracy loss. FAISS supports Product Quantization (PQ) and LSQ (including GPU).

        ### Building a Memory-Efficient Index with Product Quantization

        ```python
        # Example: IVF with Product Quantization
        nlist = 100
        m = 8  # Number of subquantizers (tune for your dimension)
        nbits = 8  # Bits per quantizer
        quantizer = faiss.IndexFlatL2(dimension)
        index_ivfpq = faiss.IndexIVFPQ(quantizer, dimension, nlist, m, nbits)

        index_ivfpq.train(embeddings)
        index_ivfpq.add(embeddings)

        ```

        Quantization enables billion-scale search on single servers or GPUs through dramatic memory reduction.

        1. **Sharding and Distributed Search**

        Massive datasets require splitting across machines (sharding). Each shard handles partial data; results merge at query time. While FAISS lacks native multi-node orchestration, it integrates with distributed databases like Pinecone, Weaviate, or Milvus.

        1. **Hybrid Search: Semantic + Keyword**

        Modern search combines vectors (semantic) with keywords (e.g., Elasticsearch). Common pattern: keyword filtering, then FAISS ranking. This hybrid maximizes precision and recall.

        1. **Integrating FAISS with Hugging Face Datasets**

        Hugging Face Datasets offers built-in FAISS integration—add indices to embedding columns for seamless search.

        ### Integrating FAISS with Hugging Face Datasets

        ```python
        from datasets import Dataset
        import numpy as np

        # Assume 'texts' (list of strings) and 'embeddings' (NumPy array) are defined
        hf_dataset = Dataset.from_dict({'text': texts, 'embeddings': list(embeddings)})

        # Add a FAISS index to the 'embeddings' column
        hf_dataset.add_faiss_index(column='embeddings')

        query_embedding = model.encode(["How can I get my money back?"]).astype(np.float32)
        scores, retrieved_examples = hf_dataset.get_nearest_examples(
            'embeddings', query_embedding, k=3
        )

        for idx, (score, example) in enumerate(zip(scores, retrieved_examples['text'])):
            print(f"Rank {idx+1}: {example} (Score: {score})")

        ```

        **Step-by-Step Explanation:**

        1. **Create Dataset**: Combine texts with their embeddings
        2. **Add FAISS Index**: Enable fast search on embedding column
        3. **Query Dataset**: Search for similar documents
        4. **Retrieve Results**: Get nearest examples with scores
        5. **Display Ranked Results**: Show top matches with similarity scores

        This integration streamlines experimentation and production deployment within the Hugging Face ecosystem.

        **Summary**: Scaling semantic search requires more than speed—it demands reliable, relevant results at any scale. Choose appropriate FAISS indices, leverage quantization, integrate with distributed and hybrid tools for enterprise-grade systems.

        **Key Points:**

        - Use FAISS v1.7.2+ for scalable vector search with latest features
        - Select exact or approximate indices based on size and latency needs
        - Apply quantization for memory efficiency on large datasets
        - Save and version indices for reliability
        - Integrate with distributed or hybrid search as needed

        ## Summary and Key Takeaways

        Traditional keyword search often misses user intent. Searching "How do I get a refund?" might overlook "Return Policy" documents. Semantic search solves this by understanding **meaning and context**—not just exact words.

        Transformer models convert text into embeddings: dense vectors capturing semantic essence. Models like BERT, RoBERTa, and newer options (E5, GTE) generate these embeddings. Similar meanings cluster together, regardless of wording differences.

        ### Generating Semantic Embeddings with Sentence Transformers

        ```python
        # Ensure Python 3.12.9
        import sys
        print(f"Python: {sys.version}")

        from sentence_transformers import SentenceTransformer
        import numpy as np

        # Load a pre-trained, efficient sentence transformer model
        model = SentenceTransformer('all-MiniLM-L6-v2')  # For production, benchmark newer models like E5 or GTE

        # Example sentences
        sentences = [
            "How do I get a refund?",
            "What is your return policy?",
            "How can I reset my password?"
        ]

        # Generate embeddings
        embeddings = model.encode(sentences)
        print(embeddings.shape)  # Output: (3, 384)

        ```

        **Step-by-Step Explanation:**

        1. **Verify Environment**: Confirm Python 3.12.9 is active
        2. **Import Libraries**: Load sentence transformers and NumPy
        3. **Initialize Model**: Load efficient pre-trained transformer
        4. **Define Sentences**: Create sample texts with varying topics
        5. **Generate Embeddings**: Convert to 384-dimensional vectors
        6. **Verify Output**: Confirm embedding dimensions

        Each sentence maps to a 384-dimensional vector. Similar meanings produce nearby embeddings in vector space—the foundation of semantic search.

        For applications with thousands of documents, efficient retrieval proves critical. FAISS enables fast, scalable vector search. Production often uses managed databases like Pinecone, Weaviate, Qdrant, or Milvus for distributed querying.

        ### Building and Querying a FAISS Index

        ```python
        import faiss

        # FAISS requires float32 arrays
        embeddings = embeddings.astype('float32')
        dimension = embeddings.shape[1]

        # For small datasets, use IndexFlatL2 (exact search)
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)  # Add document embeddings

        # For large-scale (millions of vectors), consider compressed or inverted indices:
        # index = faiss.IndexIVFPQ(faiss.IndexFlatL2(dimension), dimension, nlist=100, m=8, nbits=8)
        # See FAISS docs for details.

        # Encode a user query
        query = model.encode(["How do I get my money back?"]).astype('float32')

        # Search for the top-2 most similar documents
        D, I = index.search(query, k=2)
        print(I)  # Indices of the most relevant documents

        ```

        **Step-by-Step Explanation:**

        1. **Convert to Float32**: FAISS requires specific data type
        2. **Create Index**: Initialize based on embedding dimensions
        3. **Add Embeddings**: Store document vectors in index
        4. **Encode Query**: Transform user question with same model
        5. **Search Index**: Retrieve most similar documents efficiently
        6. **Get Results**: `I` contains indices of best matches

        For production workloads, use compressed indices (e.g., `IndexIVFPQ`) reducing memory and increasing speed. Managed vector databases offer cloud-native scaling.

        **Tips**: Always use identical embedding models for documents and queries. Ensure float32 format—FAISS requirement.

        Modern systems employ **hybrid retrieval**—combining dense vectors (semantic) with keyword search (BM25, SPLADE). This approach maximizes recall and relevance for complex queries.

        Advanced applications integrate semantic search with LLMs in **Retrieval-Augmented Generation (RAG)** architectures. Retrieved documents provide context for generative models, enabling sophisticated QA systems.

        Semantic search powers intelligent chatbots, knowledge management, and discovery tools industry-wide. It improves relevance, saves time, delights users.

        Ready to experiment? Encode your own sentences—observe which cluster by meaning. For production, benchmark multiple models (E5, GTE, OpenAI embeddings) selecting best domain fit.

        **Key Concepts Checklist:**

        - ✓ Semantic search retrieves by meaning and context, not keywords
        - ✓ Transformers and embeddings represent text as semantic vectors
        - ✓ FAISS and vector databases enable scalable similarity search
        - ✓ Hybrid retrieval combines dense and sparse methods
        - ✓ RAG integrates search with LLMs for advanced applications
        - ✓ Mastery enables fine-tuning, deployment, and continuous improvement

        **Glossary:**

        - **Semantic Search:** Finds results based on meaning and context
        - **Embedding:** Dense vector capturing semantic meaning
        - **Sentence Transformer:** Model generating sentence-level embeddings
        - **FAISS:** Library for fast, scalable vector similarity search
        - **Vector Database:** Managed system for storing/querying embeddings at scale
        - **Hybrid Search:** Combines semantic and keyword retrieval
        - **RAG:** Retrieval-Augmented Generation for context-aware LLM answers
        - **Precision/Recall:** Metrics evaluating search quality

        What's next? Article 10 teaches fine-tuning transformers for your domain. Article 8 covers data preparation. Upcoming articles explore deployment, hybrid retrieval, and continuous improvement through user feedback.

        Master semantic search and modern approaches—build AI applications that truly understand users. Let's continue!

        ## Summary

        This chapter transformed you from keyword-matching to meaning-understanding. You explored how transformers and vector databases like FAISS enable modern information retrieval. Through hands-on examples, you built embeddings and implemented scalable search. Armed with these tools, you're ready to tackle real-world AI search challenges and advance into sophisticated transformer applications.

        ## Exercises

        ### Exercise 1: Compare the results of a simple keyword search versus a semantic search on a small set of FAQs. What differences do you observe in relevance?

        **Hint:** Use a list of question-answer pairs; try matching queries with both exact string matching and with sentence transformer embeddings + FAISS.

        ### Exercise 2: Generate embeddings for a set of multilingual sentences using a multilingual sentence transformer. Evaluate whether similar meanings cluster together across languages.

        **Hint:** Use 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' and visualize embeddings with PCA or t-SNE.

        ### Exercise 3: Set up a FAISS index for a collection of 1,000 short documents. Benchmark the search speed for exact and approximate (ANN) search modes.

        **Hint:** Use IndexFlatL2 for exact and IndexIVFFlat for approximate search; measure query times for both.

        ### Exercise 4: Integrate FAISS with Hugging Face Datasets to perform semantic search directly on a dataset object.

        **Hint:** Follow the code example in the chapter; use 'add_faiss_index' and test with several queries.

        ### Exercise 5: Design your own evaluation experiment: Given a set of queries and expected relevant documents, compute precision and recall for your semantic search system.

        **Hint:** Label a small test set with ground-truth matches, then compare system output to these labels to calculate metrics.
      metadata:
        extension: .md
        size_bytes: 44043
        language: markdown
    .yamlproject/config.yaml:
      content: |
        # Default configuration for YAML Project
        include_pattern: null
        exclude_pattern: null
        temp_dir: null
        backup_dir: null
        supported_extensions:
          .py: python
          .sh: bash
          .java: java
          .js: javascript
          .jsx: javascript
          .ts: typescript
          .tsx: typescript
          .html: html
          .css: css
          .md: markdown
          .yml: yaml
          .yaml: yaml
          .json: json
          .txt: text
          .go: go
          .rs: rust
          .rb: ruby
          .php: php
          .c: c
          .cpp: cpp
          .h: c
          .hpp: cpp
          .cs: csharp
          .toml: toml
          .xml: xml
          .sql: sql
          .kt: kotlin
          .swift: swift
          .dart: dart
          .r: r
          .scala: scala
          .pl: perl
          .lua: lua
          .ini: ini
          .cfg: ini
          .properties: properties
        forbidden_dirs:
          - __pycache__
          - node_modules
          - dist
          - cdk.out
          - env
          - venv
          - .venv
          - .idea
          - build
          - .git
          - .svn
          - .hg
          - .DS_Store
          - .vs
          - .vscode
          - target
          - bin
          - obj
          - out
          - Debug
          - Release
          - tmp
          - .tox
          - .pytest_cache
          - __MACOSX
          - .mypy_cache
          - tests
        outfile: project.yaml
        log_level: INFO
        max_file_size: 204800  # 200KB
        metadata_fields:
          - extension
          - size_bytes
          - language
        yaml_format:
          indent: 2
          width: 120
      metadata:
        extension: .yaml
        size_bytes: 1103
        language: yaml
    src/config.py:
      content: |
        """Configuration module for examples."""

        import os
        from pathlib import Path
        from dotenv import load_dotenv

        # Load environment variables
        load_dotenv()

        # Project paths
        PROJECT_ROOT = Path(__file__).parent.parent
        DATA_DIR = PROJECT_ROOT / "data"
        MODELS_DIR = PROJECT_ROOT / "models"

        # Create directories if they don't exist
        DATA_DIR.mkdir(exist_ok=True)
        MODELS_DIR.mkdir(exist_ok=True)

        # Model configurations
        DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "bert-base-uncased")
        BATCH_SIZE = int(os.getenv("BATCH_SIZE", "8"))
        MAX_LENGTH = int(os.getenv("MAX_LENGTH", "512"))

        # API keys (if needed)
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
        HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

        # Device configuration
        import torch

        def get_device():
            """Get the best available device."""
            if torch.backends.mps.is_available():
                return "mps"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
                
        DEVICE = get_device()
      metadata:
        extension: .py
        size_bytes: 1013
        language: python
    src/quantization.py:
      content: |
        """Quantization implementation."""

        from transformers import pipeline, AutoTokenizer, AutoModel
        import torch
        from config import get_device, DEFAULT_MODEL

        def run_quantization_examples():
            """Run quantization examples."""
            
            print(f"Loading model: {DEFAULT_MODEL}")
            device = get_device()
            print(f"Using device: {device}")
            
            # Example implementation
            tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL)
            model = AutoModel.from_pretrained(DEFAULT_MODEL)
            
            # Example text
            text = "Hugging Face Transformers make NLP accessible to everyone!"
            
            # Tokenize
            inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
            
            print(f"\nInput text: {text}")
            print(f"Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].tolist())}")
            print(f"Token IDs: {inputs['input_ids'][0].tolist()}")
            
            # Get model outputs
            with torch.no_grad():
                outputs = model(**inputs)
            
            print(f"\nModel output shape: {outputs.last_hidden_state.shape}")
            print("Example completed successfully!")
            
        if __name__ == "__main__":
            print("=== Quantization Examples ===\n")
            run_quantization_examples()
      metadata:
        extension: .py
        size_bytes: 1189
        language: python
    src/__init__.py:
      content: |
        """
        Chapter 09 Examples: Model Optimization and Deployment
        """

        __version__ = "0.1.0"
      metadata:
        extension: .py
        size_bytes: 86
        language: python
    src/main.py:
      content: |
        """Main entry point for all examples."""

        import sys
        from pathlib import Path

        # Add src to path
        sys.path.append(str(Path(__file__).parent))

        from quantization import run_quantization_examples
        from pruning import run_pruning_examples
        from onnx_export import run_onnx_export_examples
        from performance_benchmarking import run_performance_benchmarking_examples

        def print_section(title: str):
            """Print a formatted section header."""
            print("\n" + "=" * 60)
            print(f"  {title}")
            print("=" * 60 + "\n")

        def main():
            """Run all examples."""
            print_section("CHAPTER 09: MODEL OPTIMIZATION AND DEPLOYMENT")
            print("Welcome! This script demonstrates the concepts from this chapter.")
            print("Each example builds on the previous concepts.\n")
            
            print_section("1. QUANTIZATION")
            run_quantization_examples()
            
            print_section("2. PRUNING")
            run_pruning_examples()
            
            print_section("3. ONNX EXPORT")
            run_onnx_export_examples()
            
            print_section("CONCLUSION")
            print("These examples demonstrate the key concepts from this chapter.")
            print("Try modifying the code to experiment with different approaches!")

        if __name__ == "__main__":
            main()
      metadata:
        extension: .py
        size_bytes: 1199
        language: python
