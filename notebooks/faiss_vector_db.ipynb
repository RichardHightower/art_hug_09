{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Databases with FAISS\n",
    "\n",
    "This notebook demonstrates how to use FAISS (Facebook AI Similarity Search) for efficient similarity search at scale.\n",
    "\n",
    "## Overview\n",
    "\n",
    "FAISS enables efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand different FAISS index types\n",
    "- Compare performance characteristics\n",
    "- Implement scalable vector search\n",
    "- Benchmark search quality vs. speed trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to avoid tokenizers warning\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"FAISS version: {faiss.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentence transformer model\n",
    "print(\"Loading sentence transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Test the model\n",
    "test_embedding = model.encode(\"Hello, FAISS!\")\n",
    "print(f\"\\nEmbedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset\n",
    "\n",
    "Let's create a larger dataset to demonstrate FAISS capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Generate a larger dataset for FAISS demonstration\nnp.random.seed(42)\n\n# Create synthetic documents\nnum_documents = 1000\ncategories = ['tech', 'health', 'finance', 'education', 'travel']\nsynthetic_docs = []\n\ntemplates = {\n    'tech': ['software', 'hardware', 'programming', 'AI', 'data'],\n    'health': ['wellness', 'medicine', 'fitness', 'nutrition', 'mental'],\n    'finance': ['investment', 'banking', 'budget', 'savings', 'credit'],\n    'education': ['learning', 'teaching', 'courses', 'degree', 'skills'],\n    'travel': ['vacation', 'destination', 'flights', 'hotels', 'adventure']\n}\n\nfor i in range(num_documents):\n    cat = np.random.choice(categories)\n    word = np.random.choice(templates[cat])\n    synthetic_docs.append(f\"Document about {word} in {cat} category #{i}\")\n\nprint(f\"Generated {len(synthetic_docs)} synthetic documents\")\nprint(\"\\nSample documents:\")\nfor i in range(5):\n    print(f\"  - {synthetic_docs[i]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare different FAISS index types\nimport time\n\n# Check if we have the necessary variables\nif 'doc_embeddings' not in globals() or 'synthetic_docs' not in globals():\n    print(\"⚠️ Required data not found. Let's create it now...\")\n    \n    # Generate synthetic documents if not already created\n    if 'synthetic_docs' not in globals():\n        np.random.seed(42)\n        num_documents = 1000\n        categories = ['tech', 'health', 'finance', 'education', 'travel']\n        synthetic_docs = []\n        \n        templates = {\n            'tech': ['software', 'hardware', 'programming', 'AI', 'data'],\n            'health': ['wellness', 'medicine', 'fitness', 'nutrition', 'mental'],\n            'finance': ['investment', 'banking', 'budget', 'savings', 'credit'],\n            'education': ['learning', 'teaching', 'courses', 'degree', 'skills'],\n            'travel': ['vacation', 'destination', 'flights', 'hotels', 'adventure']\n        }\n        \n        for i in range(num_documents):\n            cat = np.random.choice(categories)\n            word = np.random.choice(templates[cat])\n            synthetic_docs.append(f\"Document about {word} in {cat} category #{i}\")\n    \n    # Generate embeddings if not already created\n    if 'doc_embeddings' not in globals():\n        print(\"Generating embeddings...\")\n        if 'model' not in globals():\n            model = SentenceTransformer('all-MiniLM-L6-v2')\n        doc_embeddings = model.encode(synthetic_docs, batch_size=32, show_progress_bar=True)\n        doc_embeddings = doc_embeddings.astype('float32')\n        print(f\"Generated embeddings with shape: {doc_embeddings.shape}\")\n\n# Now proceed with the index comparison\ndimension = doc_embeddings.shape[1]\nindices = {}\nbuild_times = {}\n\n# 1. Exact search (IndexFlatL2)\nstart = time.time()\nindex_flat = faiss.IndexFlatL2(dimension)\nindex_flat.add(doc_embeddings)\nbuild_times['Flat L2 (Exact)'] = time.time() - start\nindices['Flat L2 (Exact)'] = index_flat\n\n# 2. Approximate search (IndexIVFFlat)\nstart = time.time()\nnlist = 50  # Number of clusters\nquantizer = faiss.IndexFlatL2(dimension)\nindex_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist)\nindex_ivf.train(doc_embeddings)  # Training required\nindex_ivf.add(doc_embeddings)\nbuild_times['IVF Flat (Approximate)'] = time.time() - start\nindices['IVF Flat (Approximate)'] = index_ivf\n\n# 3. Graph-based search (IndexHNSWFlat)\nstart = time.time()\nindex_hnsw = faiss.IndexHNSWFlat(dimension, 32)  # 32 is the connectivity parameter\nindex_hnsw.add(doc_embeddings)\nbuild_times['HNSW (Graph)'] = time.time() - start\nindices['HNSW (Graph)'] = index_hnsw\n\nprint(\"Index Build Times:\")\nfor name, time_taken in build_times.items():\n    print(f\"  {name}: {time_taken:.3f} seconds\")"
  },
  {
   "cell_type": "code",
   "source": "# Generate embeddings for all documents\nprint(\"Generating embeddings...\")\ndoc_embeddings = model.encode(synthetic_docs, batch_size=32, show_progress_bar=True)\ndoc_embeddings = doc_embeddings.astype('float32')  # FAISS requires float32\n\nprint(f\"\\nEmbeddings shape: {doc_embeddings.shape}\")\nprint(f\"Memory usage: {doc_embeddings.nbytes / 1024 / 1024:.2f} MB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Benchmark search performance\nif 'indices' not in globals() or not indices:\n    print(\"⚠️ Indices not found. Please run the previous cell first to build indices!\")\nelif 'model' not in globals():\n    print(\"⚠️ Model not found. Loading model...\")\n    model = SentenceTransformer('all-MiniLM-L6-v2')\nelse:\n    query = \"Looking for AI and machine learning resources\"\n    query_embedding = model.encode([query]).astype('float32')\n    k = 10  # Number of neighbors\n\n    search_times = {}\n    search_results = {}\n\n    for name, index in indices.items():\n        # Set search parameters for IVF\n        if 'IVF' in name:\n            index.nprobe = 10  # Number of clusters to search\n        \n        # Perform search\n        start = time.time()\n        distances, indices_found = index.search(query_embedding, k)\n        search_times[name] = (time.time() - start) * 1000  # Convert to ms\n        search_results[name] = (distances[0], indices_found[0])\n\n    # Visualize performance comparison\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Build times\n    names = list(build_times.keys())\n    build_values = list(build_times.values())\n    ax1.bar(names, build_values, color=['green', 'orange', 'blue'])\n    ax1.set_title('Index Build Time Comparison', fontsize=14, fontweight='bold')\n    ax1.set_ylabel('Time (seconds)')\n    ax1.tick_params(axis='x', rotation=45)\n\n    # Search times\n    search_values = list(search_times.values())\n    ax2.bar(names, search_values, color=['green', 'orange', 'blue'])\n    ax2.set_title('Search Time Comparison', fontsize=14, fontweight='bold')\n    ax2.set_ylabel('Time (milliseconds)')\n    ax2.tick_params(axis='x', rotation=45)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Display search results\n    print(f\"\\nQuery: '{query}'\")\n    print(\"\\nTop 5 results from each index type:\")\n    for name, (distances, indices_found) in search_results.items():\n        print(f\"\\n{name}:\")\n        for i in range(5):\n            idx = indices_found[i]\n            dist = distances[i]\n            print(f\"  {i+1}. {synthetic_docs[idx][:60]}... (distance: {dist:.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different FAISS Index Types\n",
    "\n",
    "FAISS provides several index types with different trade-offs between search quality and speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different FAISS index types\n",
    "dimension = doc_embeddings.shape[1]\n",
    "indices = {}\n",
    "build_times = {}\n",
    "\n",
    "# 1. Exact search (IndexFlatL2)\n",
    "start = time.time()\n",
    "index_flat = faiss.IndexFlatL2(dimension)\n",
    "index_flat.add(doc_embeddings)\n",
    "build_times['Flat L2 (Exact)'] = time.time() - start\n",
    "indices['Flat L2 (Exact)'] = index_flat\n",
    "\n",
    "# 2. Approximate search (IndexIVFFlat)\n",
    "start = time.time()\n",
    "nlist = 50  # Number of clusters\n",
    "quantizer = faiss.IndexFlatL2(dimension)\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "index_ivf.train(doc_embeddings)  # Training required\n",
    "index_ivf.add(doc_embeddings)\n",
    "build_times['IVF Flat (Approximate)'] = time.time() - start\n",
    "indices['IVF Flat (Approximate)'] = index_ivf\n",
    "\n",
    "# 3. Graph-based search (IndexHNSWFlat)\n",
    "start = time.time()\n",
    "index_hnsw = faiss.IndexHNSWFlat(dimension, 32)  # 32 is the connectivity parameter\n",
    "index_hnsw.add(doc_embeddings)\n",
    "build_times['HNSW (Graph)'] = time.time() - start\n",
    "indices['HNSW (Graph)'] = index_hnsw\n",
    "\n",
    "print(\"Index Build Times:\")\n",
    "for name, time_taken in build_times.items():\n",
    "    print(f\"  {name}: {time_taken:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Search Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark search performance\n",
    "query = \"Looking for AI and machine learning resources\"\n",
    "query_embedding = model.encode([query]).astype('float32')\n",
    "k = 10  # Number of neighbors\n",
    "\n",
    "search_times = {}\n",
    "search_results = {}\n",
    "\n",
    "for name, index in indices.items():\n",
    "    # Set search parameters for IVF\n",
    "    if 'IVF' in name:\n",
    "        index.nprobe = 10  # Number of clusters to search\n",
    "    \n",
    "    # Perform search\n",
    "    start = time.time()\n",
    "    distances, indices_found = index.search(query_embedding, k)\n",
    "    search_times[name] = (time.time() - start) * 1000  # Convert to ms\n",
    "    search_results[name] = (distances[0], indices_found[0])\n",
    "\n",
    "# Visualize performance comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Build times\n",
    "names = list(build_times.keys())\n",
    "build_values = list(build_times.values())\n",
    "ax1.bar(names, build_values, color=['green', 'orange', 'blue'])\n",
    "ax1.set_title('Index Build Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Search times\n",
    "search_values = list(search_times.values())\n",
    "ax2.bar(names, search_values, color=['green', 'orange', 'blue'])\n",
    "ax2.set_title('Search Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Time (milliseconds)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display search results\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(\"\\nTop 5 results from each index type:\")\n",
    "for name, (distances, indices_found) in search_results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for i in range(5):\n",
    "        idx = indices_found[i]\n",
    "        dist = distances[i]\n",
    "        print(f\"  {i+1}. {synthetic_docs[idx][:60]}... (distance: {dist:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced FAISS Features\n",
    "\n",
    "### 1. Index with Product Quantization (Memory Efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a memory-efficient index using Product Quantization\n",
    "# This reduces memory usage at the cost of some accuracy\n",
    "\n",
    "# Parameters\n",
    "nlist = 50  # Number of clusters\n",
    "m = 8       # Number of subquantizers\n",
    "nbits = 8   # Bits per subquantizer\n",
    "\n",
    "# Create index\n",
    "quantizer = faiss.IndexFlatL2(dimension)\n",
    "index_pq = faiss.IndexIVFPQ(quantizer, dimension, nlist, m, nbits)\n",
    "\n",
    "# Train the index\n",
    "print(\"Training PQ index...\")\n",
    "index_pq.train(doc_embeddings)\n",
    "index_pq.add(doc_embeddings)\n",
    "\n",
    "# Compare memory usage\n",
    "flat_memory = index_flat.ntotal * dimension * 4 / (1024 * 1024)  # MB\n",
    "pq_memory = index_pq.ntotal * m * nbits / 8 / (1024 * 1024)  # MB\n",
    "\n",
    "print(f\"\\nMemory Usage Comparison:\")\n",
    "print(f\"  Flat Index: {flat_memory:.2f} MB\")\n",
    "print(f\"  PQ Index: {pq_memory:.2f} MB\")\n",
    "print(f\"  Compression Ratio: {flat_memory / pq_memory:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GPU Acceleration (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available for FAISS\n",
    "gpu_available = faiss.get_num_gpus() > 0\n",
    "print(f\"GPU available for FAISS: {gpu_available}\")\n",
    "\n",
    "if gpu_available:\n",
    "    # Create GPU index\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index_gpu = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "    \n",
    "    # Benchmark GPU vs CPU\n",
    "    queries = model.encode([\"test query \" + str(i) for i in range(100)]).astype('float32')\n",
    "    \n",
    "    # CPU search\n",
    "    start = time.time()\n",
    "    index_flat.search(queries, k)\n",
    "    cpu_time = time.time() - start\n",
    "    \n",
    "    # GPU search\n",
    "    start = time.time()\n",
    "    index_gpu.search(queries, k)\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"\\nBatch Search Performance (100 queries):\")\n",
    "    print(f\"  CPU: {cpu_time:.3f} seconds\")\n",
    "    print(f\"  GPU: {gpu_time:.3f} seconds\")\n",
    "    print(f\"  Speedup: {cpu_time / gpu_time:.2f}x\")\n",
    "else:\n",
    "    print(\"GPU not available - skipping GPU benchmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Index Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load FAISS indices\n",
    "import tempfile\n",
    "\n",
    "# Create a temporary directory for saving indices\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    # Save index\n",
    "    index_path = os.path.join(tmpdir, \"faiss_index.bin\")\n",
    "    faiss.write_index(index_flat, index_path)\n",
    "    print(f\"Index saved to: {index_path}\")\n",
    "    print(f\"File size: {os.path.getsize(index_path) / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Load index\n",
    "    loaded_index = faiss.read_index(index_path)\n",
    "    print(f\"\\nIndex loaded successfully\")\n",
    "    print(f\"Number of vectors: {loaded_index.ntotal}\")\n",
    "    \n",
    "    # Verify loaded index works\n",
    "    test_query = model.encode([\"test query\"]).astype('float32')\n",
    "    D, I = loaded_index.search(test_query, 5)\n",
    "    print(f\"\\nTest search on loaded index successful\")\n",
    "    print(f\"Top result: {synthetic_docs[I[0][0]][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Tips for Using FAISS\n",
    "\n",
    "### Choosing the Right Index Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision guide visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Define index characteristics\n",
    "index_types = ['IndexFlatL2', 'IndexIVFFlat', 'IndexIVFPQ', 'IndexHNSWFlat']\n",
    "characteristics = ['Search Quality', 'Search Speed', 'Memory Efficiency', 'Build Speed']\n",
    "\n",
    "# Scores (1-5 scale)\n",
    "scores = np.array([\n",
    "    [5, 1, 1, 5],  # IndexFlatL2\n",
    "    [4, 3, 3, 3],  # IndexIVFFlat\n",
    "    [3, 4, 5, 2],  # IndexIVFPQ\n",
    "    [4, 5, 2, 2],  # IndexHNSWFlat\n",
    "])\n",
    "\n",
    "# Create heatmap\n",
    "im = ax.imshow(scores.T, cmap='RdYlGn', aspect='auto', vmin=1, vmax=5)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(len(index_types)))\n",
    "ax.set_yticks(np.arange(len(characteristics)))\n",
    "ax.set_xticklabels(index_types)\n",
    "ax.set_yticklabels(characteristics)\n",
    "\n",
    "# Rotate the tick labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Score (1=Poor, 5=Excellent)', rotation=270, labelpad=20)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(index_types)):\n",
    "    for j in range(len(characteristics)):\n",
    "        text = ax.text(i, j, scores[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "ax.set_title('FAISS Index Type Comparison', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Index Selection Guide:\")\n",
    "print(\"• Small dataset (<10K vectors): Use IndexFlatL2 for exact search\")\n",
    "print(\"• Medium dataset (10K-1M): Use IndexIVFFlat for good balance\")\n",
    "print(\"• Large dataset with memory constraints: Use IndexIVFPQ\")\n",
    "print(\"• Need very fast search: Use IndexHNSWFlat (but uses more memory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Example: Document Search System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a more realistic document search system\nclass FAISSDocumentSearch:\n    def __init__(self, model_name='all-MiniLM-L6-v2'):\n        self.model = SentenceTransformer(model_name)\n        self.index = None\n        self.documents = []\n        \n    def index_documents(self, documents, index_type='IVFFlat', nlist=None):\n        \"\"\"Index documents using specified FAISS index type\"\"\"\n        self.documents = documents\n        \n        # Generate embeddings\n        print(f\"Generating embeddings for {len(documents)} documents...\")\n        embeddings = self.model.encode(documents, show_progress_bar=True)\n        embeddings = embeddings.astype('float32')\n        \n        dimension = embeddings.shape[1]\n        n_documents = len(documents)\n        \n        # Auto-adjust nlist if not provided\n        if nlist is None:\n            # Rule of thumb: sqrt(n) clusters, but at least 1 and no more than n_documents\n            nlist = max(1, min(int(np.sqrt(n_documents)), n_documents))\n            if index_type == 'IVFFlat':\n                print(f\"Auto-adjusted nlist to {nlist} based on {n_documents} documents\")\n        \n        # Create index based on type\n        if index_type == 'Flat':\n            self.index = faiss.IndexFlatL2(dimension)\n            self.index.add(embeddings)\n        elif index_type == 'IVFFlat':\n            # For small datasets, fall back to Flat index\n            if n_documents < 40:\n                print(f\"⚠️ Only {n_documents} documents. Using Flat index instead of IVF for better results.\")\n                self.index = faiss.IndexFlatL2(dimension)\n                self.index.add(embeddings)\n            else:\n                quantizer = faiss.IndexFlatL2(dimension)\n                self.index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n                self.index.train(embeddings)\n                self.index.add(embeddings)\n        elif index_type == 'HNSW':\n            self.index = faiss.IndexHNSWFlat(dimension, 32)\n            self.index.add(embeddings)\n        \n        print(f\"Indexed {self.index.ntotal} documents\")\n        \n    def search(self, query, k=5):\n        \"\"\"Search for similar documents\"\"\"\n        # Generate query embedding\n        query_embedding = self.model.encode([query]).astype('float32')\n        \n        # Adjust k if we have fewer documents\n        k = min(k, len(self.documents))\n        \n        # Search\n        if hasattr(self.index, 'nprobe'):\n            self.index.nprobe = 10  # For IVF indices\n            \n        distances, indices = self.index.search(query_embedding, k)\n        \n        # Format results\n        results = []\n        for dist, idx in zip(distances[0], indices[0]):\n            results.append({\n                'document': self.documents[idx],\n                'distance': float(dist),\n                'similarity': 1 / (1 + float(dist))  # Convert distance to similarity\n            })\n        \n        return results\n\n# Create example documents\nexample_docs = [\n    \"Python is a versatile programming language used for web development.\",\n    \"Machine learning algorithms can predict future trends from historical data.\",\n    \"Natural language processing helps computers understand human language.\",\n    \"Deep learning neural networks are inspired by the human brain.\",\n    \"Data science combines statistics, programming, and domain knowledge.\",\n    \"Cloud computing provides on-demand access to computing resources.\",\n    \"Cybersecurity protects systems and networks from digital attacks.\",\n    \"DevOps practices combine software development and IT operations.\",\n    \"Blockchain technology enables secure, decentralized transactions.\",\n    \"Artificial intelligence aims to create intelligent machines.\"\n]\n\n# Initialize and test the system\nsearch_system = FAISSDocumentSearch()\nsearch_system.index_documents(example_docs, index_type='IVFFlat')\n\n# Test queries\ntest_queries = [\n    \"How does AI work?\",\n    \"Security best practices\",\n    \"Programming for beginners\"\n]\n\nfor query in test_queries:\n    print(f\"\\n🔍 Query: '{query}'\")\n    results = search_system.search(query, k=3)\n    for i, result in enumerate(results, 1):\n        print(f\"  {i}. {result['document'][:70]}...\")\n        print(f\"     Similarity: {result['similarity']:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "FAISS is a powerful library for similarity search that enables:\n",
    "\n",
    "1. **Scalable Search**: Handle millions or billions of vectors\n",
    "2. **Flexible Trade-offs**: Choose between accuracy and speed\n",
    "3. **Memory Efficiency**: Use quantization for large-scale deployments\n",
    "4. **GPU Acceleration**: Leverage GPUs for faster search\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **IndexFlatL2**: Best for small datasets requiring exact search\n",
    "- **IndexIVFFlat**: Good balance for medium-sized datasets\n",
    "- **IndexIVFPQ**: Memory-efficient for large datasets\n",
    "- **IndexHNSWFlat**: Fast search with graph-based approach\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Experiment with different index types for your use case\n",
    "- Try GPU acceleration for larger datasets\n",
    "- Explore advanced features like index sharding\n",
    "- Combine FAISS with filtering for hybrid search\n",
    "\n",
    "For more information, visit the [FAISS documentation](https://github.com/facebookresearch/faiss/wiki)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}